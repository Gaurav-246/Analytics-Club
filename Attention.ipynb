{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq model with Attention in sentiment analysis :"
      ],
      "metadata": {
        "id": "GjPWwsaeu_PO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries :"
      ],
      "metadata": {
        "id": "yPzoiHRlxrxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "e4CcM3Wxu9eA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "from keras.datasets import imdb\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data :"
      ],
      "metadata": {
        "id": "bMu4cbk6x4dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_unique_words = 10000\n",
        "(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words=n_unique_words)"
      ],
      "metadata": {
        "id": "ZLjZVbtCx36L"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 100\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "GY9yXxQnyQUE"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial LSTM model (bi-directional) without attention :"
      ],
      "metadata": {
        "id": "i4cSfFruyZ-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(n_unique_words, 128, input_length=maxlen))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2wHSk05yZp1",
        "outputId": "e4188fce-7e72-4f6a-e7c8-0a30101c312b"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_35\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_35 (Embedding)    (None, 100, 128)          1280000   \n",
            "                                                                 \n",
            " bidirectional_33 (Bidirecti  (None, 128)              98816     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,378,945\n",
            "Trainable params: 1,378,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history1=model.fit(x_train, y_train, batch_size=256, epochs=12)\n",
        "train_mse_wo_attn = model.evaluate(x_train, y_train)\n",
        "test_mse_wo_attn = model.evaluate(x_test, y_test)\n",
        "print(\"Train set MSE without attention = \", train_mse_wo_attn)\n",
        "print(\"Test set MSE without attention = \", test_mse_wo_attn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_N63QQ9zHp5",
        "outputId": "037f602b-1081-4178-ef58-d12969a92039"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "98/98 [==============================] - 5s 17ms/step - loss: 0.5059 - accuracy: 0.7416\n",
            "Epoch 2/12\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.2811 - accuracy: 0.8858\n",
            "Epoch 3/12\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.2203 - accuracy: 0.9145\n",
            "Epoch 4/12\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.1785 - accuracy: 0.9359\n",
            "Epoch 5/12\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.1417 - accuracy: 0.9507\n",
            "Epoch 6/12\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.1150 - accuracy: 0.9604\n",
            "Epoch 7/12\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.0829 - accuracy: 0.9728\n",
            "Epoch 8/12\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.0731 - accuracy: 0.9760\n",
            "Epoch 9/12\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.0545 - accuracy: 0.9834\n",
            "Epoch 10/12\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.0527 - accuracy: 0.9834\n",
            "Epoch 11/12\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.0400 - accuracy: 0.9876\n",
            "Epoch 12/12\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 0.0317 - accuracy: 0.9902\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.0242 - accuracy: 0.9955\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 0.7123 - accuracy: 0.8228\n",
            "Train set MSE without attention =  [0.024160468950867653, 0.9955199956893921]\n",
            "Test set MSE without attention =  [0.712272584438324, 0.8227599859237671]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building with Attention layer :"
      ],
      "metadata": {
        "id": "_2VBiw2A0W-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class attention(Layer):\n",
        "    def __init__(self, return_sequences=True):\n",
        "        self.return_sequences = return_sequences\n",
        "\n",
        "        super(attention,self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"zeros\")\n",
        "\n",
        "        \n",
        "        super(attention,self).build(input_shape)\n",
        "\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        output=x*at\n",
        "        return K.sum(output,axis=1)\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()\n"
      ],
      "metadata": {
        "id": "B2lQYr8ozp72"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Embedding(n_unique_words, 128, input_length=maxlen))\n",
        "model2.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model2.add(attention(return_sequences=False))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyXkgQvk2JFX",
        "outputId": "f4dd0c4c-ccf0-423f-8c4a-ca123419ad8f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_36 (Embedding)    (None, 100, 128)          1280000   \n",
            "                                                                 \n",
            " bidirectional_34 (Bidirecti  (None, 100, 128)         98816     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " attention_19 (attention)    (None, 128)               228       \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,379,173\n",
            "Trainable params: 1,379,173\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = model2.fit(x_train, y_train, batch_size=256, epochs=12)\n",
        "train_mse_attn = model2.evaluate(x_train, y_train)\n",
        "test_mse_attn = model2.evaluate(x_test, y_test)\n",
        "print(\"Train set MSE with attention = \", train_mse_attn)\n",
        "print(\"Test set MSE with attention = \", test_mse_attn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soWEAndB2g1C",
        "outputId": "5e8d6d0d-cd5a-4a2d-ef01-2502f09fc67c"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "98/98 [==============================] - 6s 32ms/step - loss: 0.5047 - accuracy: 0.7396\n",
            "Epoch 2/12\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.2384 - accuracy: 0.9088\n",
            "Epoch 3/12\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.1562 - accuracy: 0.9453\n",
            "Epoch 4/12\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.1115 - accuracy: 0.9616\n",
            "Epoch 5/12\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.0767 - accuracy: 0.9750\n",
            "Epoch 6/12\n",
            "98/98 [==============================] - 3s 30ms/step - loss: 0.0424 - accuracy: 0.9876\n",
            "Epoch 7/12\n",
            "98/98 [==============================] - 3s 30ms/step - loss: 0.0290 - accuracy: 0.9918\n",
            "Epoch 8/12\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.0318 - accuracy: 0.9902\n",
            "Epoch 9/12\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.0245 - accuracy: 0.9921\n",
            "Epoch 10/12\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.0146 - accuracy: 0.9964\n",
            "Epoch 11/12\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.0165 - accuracy: 0.9954\n",
            "Epoch 12/12\n",
            "98/98 [==============================] - 3s 31ms/step - loss: 0.0261 - accuracy: 0.9921\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.0098 - accuracy: 0.9974\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.6855 - accuracy: 0.8439\n",
            "Train set MSE with attention =  [0.009753917343914509, 0.9973599910736084]\n",
            "Test set MSE with attention =  [0.6855257749557495, 0.8439199924468994]\n"
          ]
        }
      ]
    }
  ]
}